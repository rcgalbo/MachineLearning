{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1: Titanic Survival Classification\n",
    "\n",
    "Using the Titanic dataset, create features to classify the survival of passenger\n",
    "\n",
    "![alt-text](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/450px-RMS_Titanic_3.jpg \"titanic\")\n",
    "\n",
    "\n",
    "### First and foremost, the imports\n",
    "\n",
    "Obviously one of the most important reasons people use Python is because of its codebase. If you ever have a problem coding in python, chances are somone else has solved your problem better than you probably could have and  has shared reproducable code that is most likely awesome!\n",
    "\n",
    "So I prefer to use what other people have done becuase I am lazy.\n",
    "\n",
    "**The packages I will be using for this homework:**\n",
    "- **Pandas**\n",
    "    - This is my favorite python package for data manipulation. Pandas has saved me so much time it is unbelievable. I highly recommend if you plan on using Python for data science.\n",
    "    - http://pandas.pydata.org/pandas-docs/stable/\n",
    "- **Numpy**\n",
    "- **Scikit-Learn (sklearn)**\n",
    "    - Preprogrammed ml so your life is easier.\n",
    "    - http://scikit-learn.org/stable/user_guide.html\n",
    "- **Matplotlib / Seaborn**\n",
    "    - Matplotlib is easy matlab style graphing in Python but truth be told, it's ugly as shit so just install seaborn and it well render overlay that looks wayyy better. \n",
    "    - https://stanford.edu/~mwaskom/software/seaborn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data with pandas\n",
    "\n",
    "I have the titanic training data saved in the same directory as this notebook so loading the data into a pandas DataFrame is really really easy, like one line of code easy. A pandas data frame is like a fancy overlay for numpy arrays that allows you to construct a heterogenous array containing all types.\n",
    "\n",
    "Below is two lines of code that imports and displays the first 5 lines of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "1                   0       3   \n",
       "2                   1       1   \n",
       "3                   1       3   \n",
       "4                   1       1   \n",
       "5                   0       3   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
       "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "5                                     Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                          \n",
       "1                1      0         A/5 21171   7.2500   NaN        S  \n",
       "2                1      0          PC 17599  71.2833   C85        C  \n",
       "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4                1      0            113803  53.1000  C123        S  \n",
       "5                0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = pd.read_csv('train.csv', index_col=[0])\n",
    "tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah so as you can see, a really awesome way to load in data.\n",
    "\n",
    "Now we have a data frame in python that we can manipulate to clean this data so that we can pass it into a classifierb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 1 to 891\n",
      "Data columns (total 11 columns):\n",
      "Survived    891 non-null int64\n",
      "Pclass      891 non-null int64\n",
      "Name        891 non-null object\n",
      "Sex         891 non-null object\n",
      "Age         714 non-null float64\n",
      "SibSp       891 non-null int64\n",
      "Parch       891 non-null int64\n",
      "Ticket      891 non-null object\n",
      "Fare        891 non-null float64\n",
      "Cabin       204 non-null object\n",
      "Embarked    889 non-null object\n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 83.5+ KB\n"
     ]
    }
   ],
   "source": [
    "#lets look at the nan values present and figure out what to do with those\n",
    "tr.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that there are NA values present in Age, Cabin and Embarked\n",
    "\n",
    "When building predictive models, it is often important to deal with NA's in an inteligent way. Since there aren't many NA's in the data and it is still early in the class, I will simply impute the median for Age and the most common value for Embarked. Embarked won't be an issue because there are only two missing, age will definitly be impacted and definitly devalue its predictive ability.\n",
    "\n",
    "I will be dealing with the Cabin variable separately so we are just filling Age and Embarked.\n",
    "\n",
    "** Note, this isn't the proper way to deal with missing data and there is an entire field of statistics that is dedicated to this matter.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Fill the age values with the median\n",
    "train = tr.copy()\n",
    "train.Age = tr.Age.fillna(np.median(train.Age.dropna()))\n",
    "\n",
    "\n",
    "#fill the embarked value with the most frequent\n",
    "embarked_val = tr.Embarked.dropna().value_counts().idxmax()\n",
    "train.Embarked = tr.Embarked.replace(np.nan, embarked_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas doesn't like setting vaues on only portions of a dataframe, so the common practice is to create a copy of the dataframe to set vaues on. Obviously this isn't efficient so if your data is big then don't copy and just grit your teeth and deal with the bitching that pandas is gonna provide.\n",
    "\n",
    "If you know yourself to be a careful and diligent data setter, then here is a link to a SE post that explains how to disable the warning that crops up when you try to do that.\n",
    "\n",
    "http://stackoverflow.com/questions/20625582/how-to-deal-with-settingwithcopywarning-in-pandas\n",
    "\n",
    "Here is the command: \n",
    "\n",
    "`pd.options.mode.chained_assignment = None  # default='warn'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.    ,   1.    ,   0.    ,   1.    ,   0.    ,   0.    ,\n",
       "         38.    ,  71.2833],\n",
       "       [  3.    ,   0.    ,   0.    ,   1.    ,   0.    ,   2.    ,\n",
       "         26.    ,   7.925 ],\n",
       "       [  1.    ,   1.    ,   0.    ,   1.    ,   0.    ,   2.    ,\n",
       "         35.    ,  53.1   ],\n",
       "       [  3.    ,   0.    ,   0.    ,   1.    ,   1.    ,   2.    ,\n",
       "         35.    ,   8.05  ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean up the training data to use in numpy array\n",
    "#create a vector of the response variable\n",
    "y = train.Survived.values\n",
    "\n",
    "#encode sex and embarked as numerical variables using sklearns label encoder\n",
    "#that we imported above\n",
    "le = LabelEncoder()\n",
    "sex = le.fit_transform(train.Sex)\n",
    "le1 = LabelEncoder()\n",
    "embarked = le1.fit_transform(train.Embarked)\n",
    "\n",
    "\n",
    "#grab all of the categorical variables\n",
    "cat_vars = train[['Pclass', 'SibSp', 'Parch']].values\n",
    "\n",
    "#Since the cabin feature is a little confusing lets just binarize it to\n",
    "#just include the information whether an individual was in a cabin or not\n",
    "inCabin = [0 if i == np.nan else 1 for i in train.Cabin]\n",
    "inCabin = np.array(inCabin)\n",
    "\n",
    "#lets combine all of the categorical variables\n",
    "#here using the np.insert function to add the \n",
    "#converted categorical features to the array\n",
    "\n",
    "cat_vars = np.insert(cat_vars, len(cat_vars[0]), inCabin, axis = 1)\n",
    "cat_vars = np.insert(cat_vars, len(cat_vars[0]), sex, axis=1)\n",
    "cat_vars = np.insert(cat_vars, len(cat_vars[0]), embarked, axis=1)\n",
    "\n",
    "#create new array for storing numeric variables\n",
    "num_vars = train[[\"Age\",\"Fare\"]].values\n",
    "\n",
    "#here I am adding in the numeric variables to the categorical variables, but ticket price is a float\n",
    "#converting integers to floats will not lose any information, however ticket price would suffer from\n",
    "#conversion to a float, although I suspect that wouldn't damage our results too much\n",
    "cat_vars = np.insert(cat_vars, len(cat_vars[0]), num_vars[:,0], axis=1)\n",
    "cat_vars = cat_vars.astype(float)\n",
    "cat_vars = np.insert(cat_vars, len(cat_vars[0]), num_vars[:,1], axis=1)\n",
    "\n",
    "train_array = cat_vars\n",
    "train_array[1:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So there is the head of our numpy array containing all of the data that we are going to use as features. All of the categorical variables have been encoded and the numeric variables have been added at the end.\n",
    "\n",
    "A note on the above code - *as you can see it is very repetative. It is very easy to create methods for doing all of this repetative work but methods that call fucntions from packages aren't always the easiest to demonstrate and especially difficult to present in a notebook.*\n",
    "\n",
    "This is clearley not as easy to read as the pandas version.\n",
    "\n",
    "Below I am printing a dictionary that contains the names and column locations for easier reference. If there are a large amount of features this won't be doable by hand but there are alot of ways of doing this using the `pandas.DataFame.columns` to get a list of the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Parch': 2, 'Pclass': 0, 'inCabin': 3, 'Fare': 7, 'SibSp': 1, 'Embarked': 5, 'Age': 6, 'Sex': 4}\n"
     ]
    }
   ],
   "source": [
    "feature_names = {'Pclass':0, 'SibSp':1, 'Parch':2,'inCabin':3, 'Sex':4, 'Embarked':5, 'Age':6, 'Fare':7}\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So the above section was all about cleaning up the data that was initially loaded. At this point I would either save my cleaned features to a csv or pickle the data into a precompiled python object. Essentially storing the final training numpy ndarray to disk and being able to clear my memory. \n",
    "\n",
    "Clearly this isn't large data but I will show what I mean by this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(train_array, open( \"train_array.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.98 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10000 loops, best of 3: 95.3 Âµs per loop\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  3.    ,   1.    ,   0.    ,   1.    ,   1.    ,   2.    ,\n",
       "         22.    ,   7.25  ],\n",
       "       [  1.    ,   1.    ,   0.    ,   1.    ,   0.    ,   0.    ,\n",
       "         38.    ,  71.2833],\n",
       "       [  3.    ,   0.    ,   0.    ,   1.    ,   0.    ,   2.    ,\n",
       "         26.    ,   7.925 ],\n",
       "       [  1.    ,   1.    ,   0.    ,   1.    ,   0.    ,   2.    ,\n",
       "         35.    ,  53.1   ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%timeit rain_array = pickle.load(open( \"train_array.p\", \"rb\" ))\n",
    "train_array[0:4,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 3.4 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit tr = pd.read_csv('train.csv', index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in practivce I wouldn't time how long it took but you can see here that loading a serialized object is way faster than using pandas. Than preformance advantage becomes important as the datasets become larger as well as having the extra memory.\n",
    "\n",
    "IRL I would do the cleaning/serialization in a python scrip or just restart the notebook server and load the serialized array to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Modeling the data\n",
    "\n",
    "So now that we have this trimmed down feature array loaded into memory, we can start to model our data.\n",
    "\n",
    "We talked about splitting the data set into a labeled train and test dataset. This is easily done using sklearn and I highly recomend using the `train_test_split` function from the cross_validation class. \n",
    "\n",
    "There is no hard and fast rule about hom much data to put into each partition. I usually go with 70% or 80% but depending on the amount of data and the distribution of classes you could do anywhere from half and half to 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "#we need four objects to hold the four arrays that this function is going to return\n",
    "#the split is 80% in train and 20% in test\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_array, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next step of standardization of the features is done in the book but is something that isn't always default atleast in a statistical approach. Non-parametric classifiers don't rely as heavily on underlying distributions and therefore it isn't necessary to standardize always unless you are seeing certain variables unduely dominate the classifier.\n",
    "\n",
    "Standardizing also is removing interpretability from the model. So say we were preforming some kind of logistic regression, the coeffiecnts of that regression model now wouldn't be interpretable in the same way as the non-scaled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#create a scaler object\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is split and standard we can actually start to train a prediction model.\n",
    "\n",
    "I am just going to use two very simple clasifiers that are fairly easy to understand their methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=0, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#here we are creating a logistic regression object and training it on standardized features\n",
    "lr = LogisticRegression(random_state = 0)\n",
    "lr.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[92, 18],\n",
       "       [18, 51]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#here we can evaluate \n",
    "cfm = confusion_matrix(lr.predict(X_test_std), y_test)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This array above is a very informative\n",
    "\n",
    "![alt-text](http://www.gepsoft.com/gepsoft/APS3KB/Chapter09/Section2/confusionmatrix.png \"Confusion Matrix\")\n",
    "\n",
    "Using this we can calculate our predictive accuracy and precision, but if we train a few models with different parameters we can start to evaluate these models in an objective way using the information that we get from this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    549\n",
       "1    342\n",
       "Name: Survived, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr.Survived.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.798882681564\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", (cfm[0][0] + cfm[1][1])/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocuurance of positive:  0.6161616161616161\n"
     ]
    }
   ],
   "source": [
    "#base occurance rate of positive in test set\n",
    "print(\"Ocuurance of positive: \", 549/(342+549))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here accuracy may not be the best measurement of predictive ability because the base occurance rate of the positives in the test set is roughly 60% and we saw our predictive accuracy around 80%. This is much less impressive than 80% accuray when the positives and negatives are all held even.\n",
    "\n",
    "Below I am going to train another very simple model that is even easier to understand than Logistic regression.\n",
    "\n",
    "Here we are going to train a KNN classifier that essentially finds the points that the observation is nearest too in p-space and classifies the observation to be the most common class of the k nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "\n",
    "knn = KNN(5)\n",
    "knn.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97, 20],\n",
       "       [13, 49]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfm1 = confusion_matrix(knn.predict(X_test_std), y_test)\n",
    "cfm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.815642458101\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", (cfm1[0][0] + cfm1[1][1])/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here that KNN is slightly more accurate than the Logistic regression but not by much. This comparison is ignoring a rather important part of ML which is paramater tuning or optimization of the models."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
